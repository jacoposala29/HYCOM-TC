{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badca880",
   "metadata": {},
   "source": [
    "# Create mask (inc vs dec) based on ML HYCOM data, classify data in increasing vs decreasing, calculate composites and make maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6995f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date, timedelta, datetime\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc4\n",
    "import glob\n",
    "import os.path\n",
    "import webbrowser\n",
    "import time\n",
    "import gsw\n",
    "import scipy.io as scipy\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib as mpl\n",
    "from FUNCTIONS_HYCOM import set_regions, set_regions_tags, find_timedelta64_index, \\\n",
    "create_composite, name_composite, below_above_MLD, plot_simple_pcolor, set_regions_offset, draw_box, \\\n",
    "save_3dvars_as_nc4, save_2dvars_as_nc4, save_1dvars_as_nc4, \\\n",
    "set_cmap, set_clim_plots, set_clim_plots_std, set_clim_plots_diff, set_clim_plots_std_diff, \\\n",
    "function_map_2D_hist, function_plots, function_single_panels_plots\n",
    "import pickle as pkl\n",
    "from holteandtalley import HolteAndTalley\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fa7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open netcdf with both HYCOM and forcing data from all events across all regions, at hourly resolution\n",
    "combined_regions = xr.open_dataset('combined_regions.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e2ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for mask calculation\n",
    "meters_below_ML = 50 # meters below MLD where to calculate salinity\n",
    "\n",
    "# REMOVE i_time if it's the same as index_time_start \n",
    "i_time = int((np.abs(combined_regions.delta_time - int(-2 * 24 * 60 * 60 * 1E9))).argmin()) # time index where to perform the classification (-2 days)\n",
    "\n",
    "# Needed for composite calculation & plots\n",
    "index_time_start = int((np.abs(combined_regions.delta_time - int(-2 * 24 * 60 * 60 * 1E9))).argmin()) # 2 days prior\n",
    "index_time_end = int((np.abs(combined_regions.delta_time - int(14 * 24 * 60 * 60 * 1E9))).argmin()) # 14 days after\n",
    "\n",
    "# Needed for composite plots\n",
    "offset_lon = 20\n",
    "timesteps_before = 2*24 # 2 days prior\n",
    "timesteps_after = 14*24 # 14 days after\n",
    "time_plot = np.arange(-timesteps_before,timesteps_after+1,1) * 1/24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c3cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical structure of absolute salinity, to use for classification\n",
    "data = combined_regions.salinity_abs\n",
    "# MLD to use for classification\n",
    "MLD = combined_regions.densityMLD_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0ee5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mask increasing/decreasing (based on 2 days before the event)\n",
    "# Goal: have a numpy array with True/False for increasing/decreasing, with one value for each event\n",
    "\n",
    "# Initialize list for mask\n",
    "mask_incr = []\n",
    "mask_decr = []\n",
    "\n",
    "for i_event in np.arange(data.shape[0]): # loop across events\n",
    "    \n",
    "    # MIXED LAYER\n",
    "    # Create new depth axis which includes MLD\n",
    "    i_ML_sorted_depth = np.searchsorted(data[i_event].depth.values, MLD[i_event][i_time])\n",
    "    depth_new_upper = np.insert(data[i_event].depth.values, i_ML_sorted_depth, MLD[i_event][i_time])\n",
    "    # Calculate salinity at MLD via interpolation\n",
    "    salinity_abs_upper = data[i_event][i_time].interp(depth=depth_new_upper)\n",
    "    \n",
    "    # Calculate weighted average of salinity within the ML (from surface to MLD)\n",
    "    # First, need to calculate dz (depth of each layer associated with a salinity value)\n",
    "    depth_new_upper = np.insert(depth_new_upper, i_ML_sorted_depth, MLD[i_event][i_time]) # add another MLD value next to the other one\n",
    "    depth_new_upper = np.insert(depth_new_upper, 0, 0) # add another zero at the beginning of the depth\n",
    "\n",
    "    dz = (depth_new_upper[2:i_ML_sorted_depth+3]-depth_new_upper[0:i_ML_sorted_depth+1])/2 # calculate dz as half of the difference between depth value above and below each depth\n",
    "#     dz[-1] = (depth_new_upper[i_ML_sorted_depth+1]-depth_new_upper[i_ML_sorted_depth])/2\n",
    "    MLS_abs_wgt = (np.sum(dz * salinity_abs_upper[0:i_ML_sorted_depth+1]))/np.sum(dz) # weighted average from surface to MLD\n",
    "\n",
    "    # LAYER BELOW ML\n",
    "    # Create new depth axis which includes meters_below_ML below MLD\n",
    "    i_ML_below_sorted_depth = np.searchsorted(data[i_event].depth.values, MLD[i_event][i_time] + meters_below_ML)\n",
    "    depth_new_lower = np.insert(data[i_event].depth.values, i_ML_below_sorted_depth, MLD[i_event][i_time] + meters_below_ML)\n",
    "    # Calculate salinity at meters_below_ML below MLD via interpolation\n",
    "    salinity_abs_lower = data[i_event][i_time].interp(depth=depth_new_lower)\n",
    "    \n",
    "    # Calculate difference between salinity in layer below ML and weighted average of salinity in the ML\n",
    "    mean_salinity_diff_below = np.mean(salinity_abs_lower - MLS_abs_wgt)\n",
    "\n",
    "    # If mean_salinity_diff_below > 0 --> increasing\n",
    "    if mean_salinity_diff_below >= 0: \n",
    "        mask_incr.append(1)\n",
    "        mask_decr.append(0)\n",
    "    # If mean_salinity_diff_below < 0 --> decreasing\n",
    "    else: \n",
    "        mask_incr.append(0)\n",
    "        mask_decr.append(1)                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dz = (depth_new_upper[2:i_ML_sorted_depth+3]-depth_new_upper[0:i_ML_sorted_depth+1])/2 # calculate dz as half of the difference between depth value above and below each depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c107a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format to make it consistent with old version of the code\n",
    "mask_incr = np.ravel(mask_incr).astype(bool)\n",
    "mask_decr = np.ravel(mask_decr).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a69ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List variable names\n",
    "data = ['temperature_abs_all', 'salinity_abs_all', 'density_abs_all', \\\n",
    "        'temperature_abs_minus2_all', 'salinity_abs_minus2_all', 'density_abs_minus2_all', \\\n",
    "        'temperature_anom_minus2_all', 'salinity_anom_minus2_all', 'density_anom_minus2_all']\n",
    "\n",
    "# Cases for selection\n",
    "data_mask = ['mask_incr',\n",
    "             'mask_decr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414dd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract variables from combined_regions\n",
    "# Save only from -2 days to +14 days\n",
    "temperature_abs_all = combined_regions.temperature_abs[:,index_time_start:index_time_end+1,:].values\n",
    "salinity_abs_all =  combined_regions.salinity_abs[:,index_time_start:index_time_end+1,:].values\n",
    "density_abs_all =  combined_regions.density_abs[:,index_time_start:index_time_end+1,:].values\n",
    "temperature_abs_minus2_all =  combined_regions.temperature_abs_minus2[:,index_time_start:index_time_end+1,:].values\n",
    "salinity_abs_minus2_all =  combined_regions.salinity_abs_minus2[:,index_time_start:index_time_end+1,:].values\n",
    "density_abs_minus2_all =  combined_regions.density_abs_minus2[:,index_time_start:index_time_end+1,:].values\n",
    "temperature_anom_minus2_all =  combined_regions.temperature_anom_minus2[:,index_time_start:index_time_end+1,:].values\n",
    "salinity_anom_minus2_all = combined_regions.salinity_anom_minus2[:,index_time_start:index_time_end+1,:].values\n",
    "density_anom_minus2_all = combined_regions.density_anom_minus2[:,index_time_start:index_time_end+1,:].values\n",
    "\n",
    "longitudes_2d_map  = combined_regions.longitude.values\n",
    "latitudes_2d_map  = combined_regions.latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9fbc9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute composites\n",
    "for idata in data:\n",
    "    for imask in data_mask:\n",
    "        bfr_d = eval(idata)\n",
    "        # Swap axes to make consistent with older version (and functions below)\n",
    "        bfr_d = np.swapaxes(bfr_d,0,2)\n",
    "        bfr_d = np.swapaxes(bfr_d,0,1)\n",
    "        \n",
    "        print(idata)\n",
    "        bfr_mask = eval(imask)\n",
    "        print(imask)\n",
    "        print('+++++++++++++++++++++++++++++++++')\n",
    "        # Function that creates composites\n",
    "        bfr = create_composite(bfr_d, bfr_mask, longitudes_2d_map, latitudes_2d_map)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        #exec(name_composite(idata, imask) + '_filename = [file_name_all[i] for i in np.arange(len(file_name_all)) if bfr_mask[i]]')        \n",
    "        exec(name_composite(idata, imask) + ' = bfr[0]')\n",
    "        exec(name_composite(idata, imask) + '_std = bfr[1]')\n",
    "        exec(name_composite(idata, imask) + '_std_error = bfr[2]')\n",
    "        exec(name_composite(idata, imask) + '_num_composite = bfr[3]')\n",
    "        exec(name_composite(idata, imask) + '_profiles = bfr[4]') # profiles that could be saved as .pkl for GOTM 1D model\n",
    "        exec(name_composite(idata, imask) + '_long = bfr[5]') \n",
    "        exec(name_composite(idata, imask) + '_lat = bfr[6]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719be652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables list for plots\n",
    "var_list = ['salinity', 'temperature', 'density']\n",
    "\n",
    "# Type list for plots\n",
    "type_list = ['abs_minus2', 'anom_minus2']\n",
    "\n",
    "# Cases list for plots\n",
    "case_list = ['mask_incr', 'mask_decr']\n",
    "\n",
    "# Hatch list for plots - standard deviation or standard error\n",
    "type_hatch_list = ['_std_error']\n",
    "\n",
    "units_list = ['Salinity change (psu)', 'Temperature change (°C)', 'Pot. density change (kg/m$^3$)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make composite plots and 2D maps\n",
    "for icase in case_list: # masks\n",
    "    # Make 2D map - need to only create one for each mask type\n",
    "    function_map_2D_hist(eval(var_list[0] + '_' + type_list[0] + '_' + icase + '_long'), eval(var_list[0] + '_' + type_list[0] + '_' + icase + '_lat'), icase, offset_lon)\n",
    "    for ihatch in type_hatch_list: # stdev, std error\n",
    "        if ihatch == '_std_error':\n",
    "            ihatch_factor = 1.96\n",
    "            ihatch_factor_tag = ' (95% confidence limit)'\n",
    "        else:\n",
    "            ihatch_factor = 1 # change to 2 in the future\n",
    "            ihatch_factor_tag = ''\n",
    "            \n",
    "        for itype in type_list: # abs, abs_minus2, anom\n",
    "                i = 0\n",
    "                for ivar in np.arange(0,len(var_list),1): # variables\n",
    "                    iunit = units_list[i]\n",
    "                    i = i + 1\n",
    "                    num = globals()[var_list[ivar] + '_' + itype + '_' + icase + '_num_composite']\n",
    "                    # Make composite plot\n",
    "                    function_plots(data = globals()[var_list[ivar] + '_' + itype + '_' + icase], \\\n",
    "                                   data_ihatch = ihatch_factor*globals()[var_list[ivar] + '_' + itype + '_' + icase + ihatch], \\\n",
    "                                   data_tag = var_list[ivar] + '_' + itype + '_' + icase + '_mnn' + str(np.min(num)) + '_mxn' + str(np.max(num)), \\\n",
    "                                   ihatch_tag = ihatch, \\\n",
    "                                   ihatch_tag_confidence_limit = ihatch_factor_tag, \\\n",
    "                                   data_xaxis = time_plot,\\\n",
    "                                   data_yaxis = combined_regions.depth.values,\\\n",
    "                                   data_yaxis_lim = [0, 150], \\\n",
    "                                   data_vmin = set_clim_plots(var_list[ivar], itype)[0], \\\n",
    "                                   data_vmax = set_clim_plots(var_list[ivar], itype)[1], \\\n",
    "                                   std_vmin = set_clim_plots_std(var_list[ivar], itype, ihatch)[0], \\\n",
    "                                   std_vmax = set_clim_plots_std(var_list[ivar], itype, ihatch)[1], \\\n",
    "                                   cmap = set_cmap(itype)) \n",
    "                    \n",
    "                    # SINGLE PANEL plots\n",
    "                    function_single_panels_plots(data = globals()[var_list[ivar] + '_' + itype + '_' + icase], \\\n",
    "                                   data_ihatch = ihatch_factor*globals()[var_list[ivar] + '_' + itype + '_' + icase + ihatch], \\\n",
    "                                   data_tag = var_list[ivar] + '_' + itype + '_' + icase + '_mnn' + str(np.min(num)) + '_mxn' + str(np.max(num)), \\\n",
    "                                   ihatch_tag = ihatch, \\\n",
    "                                   ihatch_tag_confidence_limit = ihatch_factor_tag, \\\n",
    "                                   data_xaxis = time_plot,\\\n",
    "                                   data_yaxis = combined_regions.depth.values,\\\n",
    "                                   data_yaxis_lim = [0, 150], \\\n",
    "                                   data_vmin = set_clim_plots(var_list[ivar], itype)[0], \\\n",
    "                                   data_vmax = set_clim_plots(var_list[ivar], itype)[1], \\\n",
    "                                   std_vmin = set_clim_plots_std(var_list[ivar], itype, ihatch)[0], \\\n",
    "                                   std_vmax = set_clim_plots_std(var_list[ivar], itype, ihatch)[1], \\\n",
    "                                   cmap = set_cmap(itype),\n",
    "                                   units = iunit)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdbacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b8044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
